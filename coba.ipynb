{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "from jaro import jaro_winkler_metric\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_colon_number(text):\n",
    "    pattern = r\"\\s*:\\s*\\d+\"\n",
    "    return re.sub(pattern, \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diacritics(text):\n",
    "    cleaned_text = unidecode(' '.join(text.split()))\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s-]', '', cleaned_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_space_between_number_and_alphabet(text):\n",
    "    pattern = r'(\\d)([a-zA-Z])|([a-zA-Z])(\\d)'\n",
    "    result = re.sub(pattern, r'\\1\\3 \\2\\4', text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_numbers(text):\n",
    "    # Remove leading spaces\n",
    "    text = text.strip()\n",
    "    # Define a regex pattern to match numbers with leading zeros\n",
    "    pattern = r'\\b0+(\\d+)\\b'\n",
    "    # Use a lambda function to remove leading zeros and substitute in the text\n",
    "    result = re.sub(pattern, lambda x: x.group(1), text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix Typo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function finds the closest match to a given book name from a list of book names.\n",
    "def jaro_function(bookname_input, booknames):\n",
    "    max_score = 0.75  # Threshold for similarity score\n",
    "    current_bookname = bookname_input  # Default value is the input book name\n",
    "\n",
    "    # Iterating through the list of book names to find the closest match\n",
    "    for bookname in booknames:\n",
    "        similarity_score = jaro_winkler_metric(bookname_input, bookname)\n",
    "        if similarity_score > max_score:\n",
    "            max_score = similarity_score\n",
    "            current_bookname = bookname\n",
    "\n",
    "    # Returning the closest match to the input book name\n",
    "    return current_bookname\n",
    "\n",
    "def fix_typo(text, app_dictionary):\n",
    "    text = text.lower()\n",
    "    text = remove_colon_number(text)\n",
    "    text = remove_diacritics(text)\n",
    "    text = add_space_between_number_and_alphabet(text)\n",
    "    text = fix_numbers(text)\n",
    "    text_list = text.split(\" \")\n",
    "    result = list()\n",
    "\n",
    "    for word in text_list:\n",
    "        current = jaro_function(word, app_dictionary)\n",
    "        if current != word:\n",
    "            pass\n",
    "        else:\n",
    "            if current in app_dictionary:\n",
    "                pass\n",
    "            else:\n",
    "                for word2 in app_dictionary:\n",
    "                    if word2 in word:\n",
    "                        current = word2\n",
    "                        break\n",
    "                    else:\n",
    "                        continue\n",
    "        result.append(current)\n",
    "    \n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"assets/classifiedchat_context.csv\")\n",
    "df = df[df['Category'] == 'report']\n",
    "\n",
    "my_dictionary = pd.read_csv(\"assets/app_dictionary.csv\")['Kamus'].tolist()\n",
    "df['Preprocessing'] = df['Message'].apply(lambda x: fix_typo(x, my_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Preprocessing'].to_csv('Preprocessing1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cases from CSV\n",
    "cases_df = pd.read_csv('assets/book_names.csv')\n",
    "\n",
    "# Extract cases as a list\n",
    "cases = cases_df['Kitab'].tolist()\n",
    "\n",
    "def preprocessing_text(report, bible_names):\n",
    "    # Convert text to lowercase\n",
    "    text = report.lower()\n",
    "    text = re.sub(r'\\s*-\\s*', '-', text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Generate regex pattern dynamically from cases to handle multiple keywords separated by spaces\n",
    "    pattern = r'\\b(?:' + '|'.join(cases) + r')(?:\\s+\\d+(?:-\\d+)?)?(?:\\s+(?:' + '|'.join(cases) + r')\\s+\\d+(?:-\\d+)?)?(?:-\\d+)?'  # Modified pattern to capture the entire phrase including hyphen\n",
    "\n",
    "    # Extract matches using regex\n",
    "    matches = re.findall(pattern, text)\n",
    "\n",
    "    # Join the matches to get the cleaned text\n",
    "    result = '-'.join(matches)\n",
    "\n",
    "    # print(\"Cleaned text:\", result)\n",
    "    return result\n",
    "\n",
    "df['Preprocessing2'] = df['Preprocessing'].apply(lambda x: preprocessing_text(x, cases))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Preprocessing2'].to_csv('Preprocessing2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "singkatan_df = pd.read_csv('assets/singkatan.csv')\n",
    "kitab_df = pd.read_csv('assets/booknames_nospace.csv')\n",
    "singkatan_dict = dict(zip(singkatan_df['Singkatan'], kitab_df['Kitab']))\n",
    "\n",
    "def ganti_singkatan(text):\n",
    "    if isinstance(text, str):  # Check if text is a string\n",
    "        words = text.split()\n",
    "        i = 0\n",
    "        while i < len(words):\n",
    "            word = words[i]\n",
    "            if '-' in word:\n",
    "                parts = word.split('-')\n",
    "                updated_parts = []\n",
    "                for part in parts:\n",
    "                    if part.lower() in singkatan_dict:  # Check lowercase for case-insensitivity\n",
    "                        updated_parts.append(singkatan_dict[part.lower()])\n",
    "                    else:\n",
    "                        updated_parts.append(part)\n",
    "                words[i] = '-'.join(updated_parts)\n",
    "            elif word.lower() in singkatan_dict:  # Check lowercase for case-insensitivity\n",
    "                words[i] = singkatan_dict[word.lower()]\n",
    "            elif i < len(words) - 1 and (word + ' ' + words[i+1]).lower() in singkatan_dict:  # Check for multi-word abbreviations\n",
    "                words[i] = singkatan_dict[(word + ' ' + words[i+1]).lower()]\n",
    "                del words[i+1]  # Remove the next word as it's part of the abbreviation\n",
    "            elif re.match(r'.*\\d', word):  # Check if the word contains a digit\n",
    "                break  # If a word with a digit is encountered, stop replacing\n",
    "            elif i > 0 and words[i-1].lower() in singkatan_dict:  # Check if the previous word is an abbreviation\n",
    "                words[i-1] = singkatan_dict[words[i-1].lower()]\n",
    "            i += 1\n",
    "        return ' '.join(words)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "processed_2 = list()\n",
    "\n",
    "for text in df['Preprocessing2']:\n",
    "    new_value = ganti_singkatan(text)\n",
    "    processed_2.append(new_value)\n",
    "\n",
    "df['Preprocessing2'] = processed_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Preprocessing2'].to_csv('Preprocessing3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_chapter_range(input_str):\n",
    "    # Split the input string into book and chapter range\n",
    "    parts = str(input_str).split()\n",
    "    if len(parts) > 2:\n",
    "        book = parts[0] + \" \" + parts[1]\n",
    "        chapters = parts[2].split('-')\n",
    "    elif len(parts) < 2:\n",
    "        return input_str\n",
    "    else:\n",
    "        book = parts[0]\n",
    "        chapters = parts[1].split('-')\n",
    "\n",
    "    # If there's only one chapter, return the original input\n",
    "    if len(chapters) == 1:\n",
    "        return input_str\n",
    "\n",
    "    # Otherwise, format the output\n",
    "    start_chapter = chapters[0]\n",
    "    end_chapter = chapters[1]\n",
    "    output = f\"{book} {start_chapter} - {book} {end_chapter}\"\n",
    "    return output\n",
    "\n",
    "\n",
    "processed3 = list()\n",
    "for i in range(len(df)):\n",
    "    new_value = expand_chapter_range(df['Preprocessing2'][i:i+1].values[0])\n",
    "    new_value = re.sub(r'(\\d+)-(\\w+)', r'\\1 - \\2', new_value)\n",
    "    processed3.append(new_value)\n",
    "    \n",
    "df['Processed 3'] = processed3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Processed 3'].to_csv('Preprocessing4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk membaca jenis kejadian dan nomor kejadian dari file CSV\n",
    "def read_events(file_name):\n",
    "    events = []\n",
    "    with open(file_name, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            events.append(row[0].lower()) \n",
    "    return events\n",
    "\n",
    "def parse_messages(df, events):\n",
    "    parsed_messages = []\n",
    "\n",
    "    for line in df['Processed 3']:\n",
    "        matches = re.findall(r'(\\w+\\s\\d+)(?:\\s*-\\s*(\\w+\\s\\d+))?', line)  # Modifikasi regex di sini\n",
    "        parsed_line = ''\n",
    "        for match in matches:\n",
    "            start_event, end_event = match[0], match[1]\n",
    "\n",
    "            start_index = events.index(start_event.lower()) if start_event.lower() in events else None\n",
    "            end_index = events.index(end_event.lower()) if end_event and end_event.lower() in events else start_index\n",
    "            if start_index is not None and end_index is not None:\n",
    "                for i in range(start_index, end_index + 1):\n",
    "                    parsed_line += f\"{events[i]}, \"  # Menghapus .capitalize() agar tidak ada huruf kapital di awal\n",
    "        parsed_line = parsed_line[:-2]  # Menghapus koma dan spasi ekstra dari akhir\n",
    "        parsed_messages.append(parsed_line)\n",
    "    \n",
    "    df['Parsed'] = parsed_messages\n",
    "\n",
    "    return df\n",
    "\n",
    "# Fungsi untuk menyimpan hasil parsing ke file\n",
    "def save_parsed_messages(parsed_messages, output_file):\n",
    "    with open(output_file, 'w') as file:\n",
    "        for parsed_line in parsed_messages:\n",
    "            file.write(parsed_line)\n",
    "\n",
    "# Contoh penggunaan\n",
    "file_name = 'assets/biblechapters_nospace.csv'\n",
    "\n",
    "events = read_events(file_name)\n",
    "df = parse_messages(df, events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Parsed'].to_csv(\"Parsed.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
