{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from jaro import jaro_winkler_metric\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean unwanted characters (,,, Kejadian 1 - 2 ,,, --> Kejadian 1 - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only report chat\n",
    "df = pd.read_csv(\"assets/classifiedchat_context.csv\")\n",
    "df = df[df['Category'] == 'report']\n",
    "\n",
    "def remove_colon_number(text):\n",
    "    pattern = r\"\\s*:\\s*\\d+\"\n",
    "    return re.sub(pattern, \"\", text)\n",
    "\n",
    "def format_numbers(report):\n",
    "    if \":\" in report:\n",
    "        pattern = r\"(\\d+)\\s*-\\s*(\\d+)(?:\\s*[,;]\\s*(\\d+)\\s*-\\s*(\\d+))*\\s*(?:done)?\"\n",
    "\n",
    "        # Using re.findall() to find all patterns in the report\n",
    "        matches = re.findall(pattern, report, re.IGNORECASE)\n",
    "\n",
    "        # If a pattern is found\n",
    "        if matches:\n",
    "            # Initializing a list to store all numbers found\n",
    "            all_numbers = []\n",
    "\n",
    "            # Collecting all numbers from each found pattern\n",
    "            for match in matches:\n",
    "                for num in match:\n",
    "                    if num:\n",
    "                        all_numbers.append(int(num))\n",
    "\n",
    "            # Sorting numbers and removing duplicates\n",
    "            all_numbers = sorted(set(all_numbers))\n",
    "\n",
    "            # Creating a range of numbers\n",
    "            min_num = all_numbers[0]\n",
    "            max_num = all_numbers[-1]\n",
    "\n",
    "            # Taking the book name from the report\n",
    "            book_name = re.search(r'^\\w+', report).group()\n",
    "\n",
    "            # Generating the result string in the desired format\n",
    "            result = f\"{book_name} {min_num}-{max_num}\"\n",
    "            return result\n",
    "        else:\n",
    "            return report\n",
    "    else:\n",
    "        return report\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'(\\w+) (\\d+), (\\d+)-(\\d+)(?:\\s\\w+)*', r'\\1 \\2-\\4', text, flags=re.IGNORECASE)\n",
    "    return cleaned_text\n",
    "\n",
    "def preprocessing_report(report):\n",
    "    # lowercase all messages\n",
    "    report = df['Preprocessing'].str.lower()\n",
    "\n",
    "    # remove unwanted characters\n",
    "    report = report.str.replace('_', '').str.replace('`', '').str.replace('.', '').str.replace('--', '-').str.replace(',', '').str.replace('*', '').str.replace('\"', '').str.lstrip().str.rstrip()\n",
    "\n",
    "    # remove all emojis\n",
    "    emojis = pd.read_csv('assets/emojis.csv')\n",
    "    emojis_list = emojis['emo'].tolist()\n",
    "\n",
    "    for emoji in emojis_list:\n",
    "        report = report.str.replace(emoji, '')\n",
    "\n",
    "    # remove unwanted words\n",
    "    word_list = ['selesai', 'done']\n",
    "    for word in word_list:\n",
    "        report = report.str.replace(word, '')\n",
    "\n",
    "    # remove unknown characters\n",
    "    for i, message in enumerate(report):\n",
    "        modified_text = re.sub(r'(?<=\\D)(\\d+)', r' \\1', message)\n",
    "        modified_text = re.sub(r'(?<=\\d)([a-zA-Z])', r' \\1', modified_text)\n",
    "        report[i:i+1] = ' '.join(modified_text.split())\n",
    "    \n",
    "    return report\n",
    "\n",
    "df['Preprocessing'] = df['Message']\n",
    "df['Preprocessing'] = df['Preprocessing'].apply(remove_colon_number)#.apply(clean_text)\n",
    "\n",
    "report = preprocessing_report(df)\n",
    "\n",
    "df['Cleaned Report'] = report.apply(format_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove Diacritic Characters (Ä·el 2-3 --> kel 2-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_and_replace_diacritics(text):\n",
    "    cleaned_text = unidecode(text)\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s-]', '', cleaned_text)\n",
    "    return cleaned_text\n",
    "\n",
    "df['Cleaned Report'] = df['Cleaned Report'].apply(remove_and_replace_diacritics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cleaned Report'].to_csv('clean_report.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix Typo in Bookname (krl 2-3, kel 2-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function corrects the typo in a given book name within a report.\n",
    "def fix_bookname_typo(report, booknames):\n",
    "    # Extracting the first word of the report\n",
    "    first_word_match = re.match(r'^\\w+', report)\n",
    "    if first_word_match:\n",
    "        first_word = first_word_match.group(0)\n",
    "    else:\n",
    "        first_word = ''  # Default value if no word is found\n",
    "       \n",
    "    # Removing the first word from the report\n",
    "    rest_of_report = re.sub(r'^\\w+', '', report).lstrip()\n",
    "\n",
    "    # Searching for a word following a hyphen in the remaining report\n",
    "    match = re.search(r'-(\\s*\\b[a-zA-Z]+\\b|\\b[a-zA-Z]+\\b)', rest_of_report)\n",
    "    if match:\n",
    "        after_hyphen = match.group(1)  # Word following the hyphen\n",
    "        # Fixing the typo in the word following the hyphen\n",
    "        fixed_after_hyphen = fix_bookname(after_hyphen.strip(), booknames)\n",
    "        # Replacing the original word with the corrected one\n",
    "        rest_of_report = rest_of_report.replace(after_hyphen, fixed_after_hyphen, 1)\n",
    "\n",
    "    # Fixing the typo in the first word of the report and returning the corrected report\n",
    "    return fix_spaces(fix_bookname(first_word, booknames) + \" \" + rest_of_report.strip())\n",
    "\n",
    "# This function finds the closest match to a given book name from a list of book names.\n",
    "def fix_bookname(bookname_input, booknames):\n",
    "    booknames_list = booknames[\"Kitab\"].tolist()  # Converting book names to a list\n",
    "\n",
    "    max_score = 0.75  # Threshold for similarity score\n",
    "    current_bookname = bookname_input  # Default value is the input book name\n",
    "\n",
    "    # Iterating through the list of book names to find the closest match\n",
    "    for bookname in booknames_list:\n",
    "        similarity_score = jaro_winkler_metric(bookname_input, bookname)\n",
    "        if similarity_score >= max_score:\n",
    "            max_score = similarity_score\n",
    "            current_bookname = bookname\n",
    "\n",
    "    # Returning the closest match to the input book name\n",
    "    return current_bookname\n",
    "\n",
    "def fix_spaces(text):\n",
    "    # Using regular expression to find hyphens surrounded by spaces\n",
    "    fixed_text = re.sub(r'\\s*-\\s*', ' - ', text)\n",
    "    return fixed_text\n",
    "\n",
    "booknames = pd.read_csv(\"assets/book_names.csv\")\n",
    "\n",
    "fix_typo = list()\n",
    "for i in range(len(df)):\n",
    "    cleaned_report_str = str(df['Cleaned Report'][i:i+1].values[0])  # Convert DataFrame slice to string\n",
    "    new_value = fix_bookname_typo(cleaned_report_str, booknames)\n",
    "    # Remove double spaces\n",
    "    output = ' '.join(new_value.split())\n",
    "    fix_typo.append(output)\n",
    "\n",
    "df['Fix Book Name'] = fix_typo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Fix Book Name'].to_csv('fix_bookname.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Just take the book name and chapter (bu saya sudah sampai kejadian 35, kejadian 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_list = pd.read_csv('assets/book_names.csv')\n",
    "book_list = book_list['Kitab'].tolist()\n",
    "\n",
    "# Construct a regex pattern to match any of the names in the list along with numbers and '-'\n",
    "pattern = r'(?:\\b(?:' + '|'.join(book_list) + r')\\b(?:\\s*\\d*-*\\d*\\s*)*)'\n",
    "\n",
    "processed_message = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    # Skip empty strings\n",
    "    text = df['Fix Book Name'][i:i+1].values[0] + \" end\"\n",
    "    if not text.strip():\n",
    "        continue\n",
    "    \n",
    "    # Find all matches for the pattern in the text\n",
    "    matches = re.findall(pattern, text)\n",
    "\n",
    "    if matches:\n",
    "        # Construct the final output by joining the matches\n",
    "        output = ' '.join(matches)\n",
    "        \n",
    "        # Remove double spaces\n",
    "        output = ' '.join(output.split())\n",
    "\n",
    "        # Remove space before or after hyphen\n",
    "        output = re.sub(r'\\s*-\\s*', r'-', output)\n",
    "    else:\n",
    "        output = text\n",
    "\n",
    "    processed_message.append(output)\n",
    "\n",
    "df['Processed'] = processed_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Processed'].to_csv('coba1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Change Abbreviation (Kel 2 - 3, Keluaran 2 - 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "singkatan_df = pd.read_csv('assets/singkatan.csv')\n",
    "kitab_df = pd.read_csv('assets/biblebooknames.csv')\n",
    "singkatan_dict = dict(zip(singkatan_df['Singkatan'], kitab_df['Kitab']))\n",
    "\n",
    "def ganti_singkatan(text):\n",
    "    if isinstance(text, str):  # Check if text is a string\n",
    "        words = text.split()\n",
    "        i = 0\n",
    "        while i < len(words):\n",
    "            word = words[i]\n",
    "            if '-' in word:\n",
    "                parts = word.split('-')\n",
    "                updated_parts = []\n",
    "                for part in parts:\n",
    "                    if part.lower() in singkatan_dict:  # Check lowercase for case-insensitivity\n",
    "                        updated_parts.append(singkatan_dict[part.lower()])\n",
    "                    else:\n",
    "                        updated_parts.append(part)\n",
    "                words[i] = '-'.join(updated_parts)\n",
    "            elif word.lower() in singkatan_dict:  # Check lowercase for case-insensitivity\n",
    "                words[i] = singkatan_dict[word.lower()]\n",
    "            elif i < len(words) - 1 and (word + ' ' + words[i+1]).lower() in singkatan_dict:  # Check for multi-word abbreviations\n",
    "                words[i] = singkatan_dict[(word + ' ' + words[i+1]).lower()]\n",
    "                del words[i+1]  # Remove the next word as it's part of the abbreviation\n",
    "            elif re.match(r'.*\\d', word):  # Check if the word contains a digit\n",
    "                break  # If a word with a digit is encountered, stop replacing\n",
    "            elif i > 0 and words[i-1].lower() in singkatan_dict:  # Check if the previous word is an abbreviation\n",
    "                words[i-1] = singkatan_dict[words[i-1].lower()]\n",
    "            i += 1\n",
    "        return ' '.join(words)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "processed_2 = list()\n",
    "\n",
    "for text in df['Processed']:\n",
    "    new_value = ganti_singkatan(text)\n",
    "    processed_2.append(new_value)\n",
    "\n",
    "df['Processed 2'] = processed_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Processed 2'] = df['Processed 2'].str.replace('--', '-')\n",
    "df['Processed 2'].to_csv('coba2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_chapter_range(input_str):\n",
    "    # Split the input string into book and chapter range\n",
    "    parts = str(input_str).split()\n",
    "    if len(parts) > 2:\n",
    "        book = parts[0] + \" \" + parts[1]\n",
    "        chapters = parts[2].split('-')\n",
    "    elif len(parts) < 2:\n",
    "        return input_str\n",
    "    else:\n",
    "        book = parts[0]\n",
    "        chapters = parts[1].split('-')\n",
    "\n",
    "    # If there's only one chapter, return the original input\n",
    "    if len(chapters) == 1:\n",
    "        return input_str\n",
    "\n",
    "    # Otherwise, format the output\n",
    "    start_chapter = chapters[0]\n",
    "    end_chapter = chapters[1]\n",
    "    output = f\"{book} {start_chapter} - {book} {end_chapter}\"\n",
    "    return output\n",
    "\n",
    "\n",
    "processed3 = list()\n",
    "for i in range(len(df)):\n",
    "    new_value = expand_chapter_range(df['Processed 2'][i:i+1].values[0])\n",
    "    processed3.append(new_value)\n",
    "\n",
    "df['Processed 3'] = processed3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Processed 3'].to_csv('coba3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk membaca jenis kejadian dan nomor kejadian dari file CSV\n",
    "def read_events(file_name):\n",
    "    events = []\n",
    "    with open(file_name, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            events.append(row[0].lower())  # Tambahkan jenis kejadian dengan nomor kejadian\n",
    "    return events\n",
    "\n",
    "# Fungsi untuk mengubah format pesan\n",
    "def parse_messages(df, events):\n",
    "    parsed_messages = []\n",
    "\n",
    "    for line in df['Processed 3']:\n",
    "        matches = re.findall(r'(\\w+\\s\\d+)\\s*-\\s*(\\w+\\s\\d+)', line)\n",
    "        parsed_line = ''\n",
    "        for match in matches:\n",
    "            start_event, end_event = match\n",
    "            start_index = events.index(start_event.lower()) if start_event.lower() in events else None\n",
    "            end_index = events.index(end_event.lower()) if end_event.lower() in events else None\n",
    "            if start_index is not None and end_index is not None:\n",
    "                for i in range(start_index, end_index + 1):\n",
    "                    parsed_line += f\"{events[i]}, \"  # Menghapus .capitalize() agar tidak ada huruf kapital di awal\n",
    "        parsed_line = parsed_line[:-2]  # Menghapus koma dan spasi ekstra dari akhir\n",
    "        parsed_messages.append(parsed_line)\n",
    "    \n",
    "    df['Parsed'] = parsed_messages\n",
    "\n",
    "    return df\n",
    "\n",
    "# Fungsi untuk menyimpan hasil parsing ke file\n",
    "def save_parsed_messages(parsed_messages, output_file):\n",
    "    with open(output_file, 'w') as file:\n",
    "        for parsed_line in parsed_messages:\n",
    "            file.write(parsed_line)\n",
    "\n",
    "# Contoh penggunaan\n",
    "file_name = 'assets/biblechapters.csv'\n",
    "\n",
    "events = read_events(file_name)\n",
    "df = parse_messages(df, events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Parsed'].to_csv(\"final.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
