{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from jaro import jaro_winkler_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only report chat\n",
    "df = pd.read_csv(\"assets/classifiedchat_context.csv\")\n",
    "cat_report = df[df['Category'] == 'report']\n",
    "\n",
    "def preprocessing_report(report):\n",
    "    # lowercase all messages\n",
    "    report = cat_report['Message'].str.lower()\n",
    "\n",
    "    # remove unwanted characters\n",
    "    report = report.str.replace('_', '').str.replace('`', '').str.replace('.', '').str.replace(',', '').str.replace('*', '').str.lstrip().str.rstrip()\n",
    "\n",
    "    # remove all emojis\n",
    "    emojis = pd.read_csv('assets/emojis.csv')\n",
    "    emojis_list = emojis['emo'].tolist()\n",
    "\n",
    "    for emoji in emojis_list:\n",
    "        report = report.str.replace(emoji, '')\n",
    "\n",
    "    # remove unwanted words\n",
    "    word_list = ['selesai', 'done']\n",
    "    for word in word_list:\n",
    "        report = report.str.replace(word, '')\n",
    "\n",
    "    # remove unknown characters\n",
    "    for i, message in enumerate(report):\n",
    "        modified_text = re.sub(r'(?<=\\D)(\\d+)', r' \\1', message)\n",
    "        modified_text = re.sub(r'(?<=\\d)([a-zA-Z])', r' \\1', modified_text)\n",
    "        report[i:i+1] = ' '.join(modified_text.split())\n",
    "    \n",
    "    return report\n",
    "\n",
    "report = preprocessing_report(cat_report)\n",
    "report.to_csv('report.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/84/tx787yms4vnbdt3_qvbbw8wh0000gn/T/ipykernel_3446/827061491.py:27: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  report2['Message'][i] = fix_bookname_typo(report2['Message'][i], booknames)\n"
     ]
    }
   ],
   "source": [
    "def fix_bookname_typo(report, booknames):\n",
    "    split_report = report.split(' ', 1)\n",
    "    bookname_input = split_report[0]\n",
    "    booknames_list = booknames[\"Kitab\"].to_list()\n",
    "\n",
    "    max_score = 0\n",
    "    current_bookname = \"\"\n",
    "\n",
    "    for bookname in booknames_list:\n",
    "        similarity_score = jaro_winkler_metric(bookname_input, bookname)\n",
    "        if similarity_score > max_score:\n",
    "            max_score = similarity_score\n",
    "            current_bookname = bookname\n",
    "        else:\n",
    "            continue\n",
    "    if len(split_report) > 1:\n",
    "        result = current_bookname + \" \" + split_report[1]\n",
    "    else:\n",
    "        result = current_bookname\n",
    "    \n",
    "    return result\n",
    "\n",
    "report2 = pd.read_csv('report.csv')\n",
    "booknames = pd.read_csv(\"assets/book_names.csv\")\n",
    "\n",
    "for i in range(len(report)):\n",
    "    report2['Message'][i] = fix_bookname_typo(report2['Message'][i], booknames)\n",
    "\n",
    "report2.to_csv('report2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_list = pd.read_csv('assets/book_names.csv')\n",
    "book_list = book_list['Kitab'].tolist()\n",
    "\n",
    "# Construct a regex pattern to match any of the names in the list along with numbers and '-'\n",
    "pattern = r'(?:\\b(?:' + '|'.join(book_list) + r')\\b(?:\\s*\\d*-*\\d*\\s*)*)'\n",
    "\n",
    "# Messages\n",
    "processed_message = []\n",
    "for text in report:\n",
    "    # Skip empty strings\n",
    "    if not text.strip():\n",
    "        continue\n",
    "    \n",
    "    # Find all matches for the pattern in the text\n",
    "    matches = re.findall(pattern, text)\n",
    "\n",
    "    # Construct the final output by joining the matches\n",
    "    output = ' '.join(matches)\n",
    "    # Remove double spaces\n",
    "    output = ' '.join(output.split())\n",
    "\n",
    "    # Remove space before or after hyphen\n",
    "    output = re.sub(r'\\s*-\\s*', r'-', output)\n",
    "\n",
    "    processed_message.append(output)\n",
    "    \n",
    "dict_msg = {'processed': processed_message}\n",
    "\n",
    "# Create DataFrame excluding empty strings\n",
    "df = pd.DataFrame(dict_msg)\n",
    "\n",
    "# Drop rows with empty strings\n",
    "# df = df[df['processed'] != \"\"]\n",
    "df.to_csv('assets/cleaned_messages.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "singkatan_df = pd.read_csv('assets/singkatan.csv')\n",
    "kitab_df = pd.read_csv('assets/biblebooknames.csv')\n",
    "singkatan_dict = dict(zip(singkatan_df['Singkatan'], kitab_df['Kitab']))\n",
    "\n",
    "# Baca file cleaned_messages.csv\n",
    "cleaned_messages_df = pd.read_csv('assets/cleaned_messages.csv')\n",
    "\n",
    "# Fungsi untuk mengganti singkatan dengan nama lengkap\n",
    "def ganti_singkatan(text):\n",
    "    words = text.split()\n",
    "    for i, word in enumerate(words):\n",
    "        if word in singkatan_dict:\n",
    "            words[i] = singkatan_dict[word]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Terapkan fungsi pada kolom 'processed' dan simpan kembali ke cleaned_messages.csv\n",
    "cleaned_messages_df['processed'] = cleaned_messages_df['processed'].apply(ganti_singkatan)\n",
    "cleaned_messages_df.to_csv('assets/cleaned_messages.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
