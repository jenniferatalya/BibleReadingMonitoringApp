{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from jaro import jaro_winkler_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Clean unwanted characters (,,, Kejadian 1 - 2 ,,, --> Kejadian 1 - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only report chat\n",
    "df = pd.read_csv(\"assets/classifiedchat_context.csv\")\n",
    "df = df[df['Category'] == 'report']\n",
    "\n",
    "def preprocessing_report(report):\n",
    "    # lowercase all messages\n",
    "    report = df['Message'].str.lower()\n",
    "\n",
    "    # remove unwanted characters\n",
    "    report = report.str.replace('_', '').str.replace('`', '').str.replace('.', '').str.replace(',', '').str.replace('*', '').str.replace('\"', '').str.lstrip().str.rstrip()\n",
    "\n",
    "    # remove all emojis\n",
    "    emojis = pd.read_csv('assets/emojis.csv')\n",
    "    emojis_list = emojis['emo'].tolist()\n",
    "\n",
    "    for emoji in emojis_list:\n",
    "        report = report.str.replace(emoji, '')\n",
    "\n",
    "    # remove unwanted words\n",
    "    word_list = ['selesai', 'done']\n",
    "    for word in word_list:\n",
    "        report = report.str.replace(word, '')\n",
    "\n",
    "    # remove unknown characters\n",
    "    for i, message in enumerate(report):\n",
    "        modified_text = re.sub(r'(?<=\\D)(\\d+)', r' \\1', message)\n",
    "        modified_text = re.sub(r'(?<=\\d)([a-zA-Z])', r' \\1', modified_text)\n",
    "        report[i:i+1] = ' '.join(modified_text.split())\n",
    "    \n",
    "    return report\n",
    "\n",
    "report = preprocessing_report(df)\n",
    "df['Cleaned Report'] = report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cleaned Report'].to_csv('clean_report.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fix Typo in Bookname (krl 2-3, kel 2-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function corrects the typo in a given book name within a report.\n",
    "def fix_bookname_typo(report, booknames):\n",
    "    # Extracting the first word of the report\n",
    "    first_word_match = re.match(r'^\\w+', report)\n",
    "    if first_word_match:\n",
    "        first_word = first_word_match.group(0)\n",
    "    else:\n",
    "        first_word = ''  # Default value if no word is found\n",
    "       \n",
    "    # Removing the first word from the report\n",
    "    rest_of_report = re.sub(r'^\\w+', '', report).lstrip()\n",
    "\n",
    "    # Searching for a word following a hyphen in the remaining report\n",
    "    match = re.search(r'-(\\s*\\b[a-zA-Z]+\\b|\\b[a-zA-Z]+\\b)', rest_of_report)\n",
    "    if match:\n",
    "        after_hyphen = match.group(1)  # Word following the hyphen\n",
    "        # Fixing the typo in the word following the hyphen\n",
    "        fixed_after_hyphen = fix_bookname(after_hyphen.strip(), booknames)\n",
    "        # Replacing the original word with the corrected one\n",
    "        rest_of_report = rest_of_report.replace(after_hyphen, fixed_after_hyphen, 1)\n",
    "\n",
    "    # Fixing the typo in the first word of the report and returning the corrected report\n",
    "    return fix_bookname(first_word, booknames) + \" \" + rest_of_report.strip()\n",
    "\n",
    "# This function finds the closest match to a given book name from a list of book names.\n",
    "def fix_bookname(bookname_input, booknames):\n",
    "    booknames_list = booknames[\"Kitab\"].tolist()  # Converting book names to a list\n",
    "\n",
    "    max_score = 0.75  # Threshold for similarity score\n",
    "    current_bookname = bookname_input  # Default value is the input book name\n",
    "\n",
    "    # Iterating through the list of book names to find the closest match\n",
    "    for bookname in booknames_list:\n",
    "        similarity_score = jaro_winkler_metric(bookname_input, bookname)\n",
    "        if similarity_score >= max_score:\n",
    "            max_score = similarity_score\n",
    "            current_bookname = bookname\n",
    "\n",
    "    # Returning the closest match to the input book name\n",
    "    return current_bookname\n",
    "\n",
    "report2 = pd.read_csv('assets/report.csv')\n",
    "booknames = pd.read_csv(\"assets/book_names.csv\")\n",
    "\n",
    "# for i in range(len(report2)):\n",
    "#     report2['Message'][i] = fix_bookname_typo(report2['Message'][i], booknames)\n",
    "\n",
    "# report2.to_csv('assets/report2.csv', index=False)\n",
    "\n",
    "fix_typo = list()\n",
    "for i in range(len(df)):\n",
    "    cleaned_report_str = str(df['Cleaned Report'][i:i+1].values[0])  # Convert DataFrame slice to string\n",
    "    new_value = fix_bookname_typo(cleaned_report_str, booknames)\n",
    "    # Remove double spaces\n",
    "    output = ' '.join(new_value.split())\n",
    "    fix_typo.append(output)\n",
    "\n",
    "df['Fix Book Name'] = fix_typo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fix_bookname_typo(report, booknames):\n",
    "#     first_word_match = re.match(r'^\\w+', report)\n",
    "#     if first_word_match:\n",
    "#         first_word = first_word_match.group(0)\n",
    "#     else:\n",
    "#         first_word = ''\n",
    "    \n",
    "#     rest_of_report = re.sub(r'^\\w+', '', report).lstrip()\n",
    "\n",
    "#     match = re.search(r'-(\\s*\\b[a-zA-Z]+\\b|\\b[a-zA-Z]+\\b)', rest_of_report)\n",
    "#     if match:\n",
    "#         after_hyphen = match.group(1)\n",
    "#         fixed_after_hyphen = fix_bookname(after_hyphen.strip(), booknames)\n",
    "#         rest_of_report = rest_of_report.replace(after_hyphen, fixed_after_hyphen, 1)\n",
    "\n",
    "#     return fix_bookname(first_word, booknames) + \" \" + rest_of_report.strip()\n",
    "\n",
    "# def fix_bookname(bookname_input, booknames):\n",
    "#     booknames_list = booknames['Kitab'].tolist()\n",
    "\n",
    "#     max_score = 0.65\n",
    "#     current_bookname = bookname_input\n",
    "\n",
    "#     for bookname in booknames_list:\n",
    "#         similarity_score = jaro_winkler_metric(bookname_input, bookname)\n",
    "#         if similarity_score > max_score:\n",
    "#             max_score = similarity_score\n",
    "#             current_bookname = bookname\n",
    "\n",
    "#     return current_bookname\n",
    "\n",
    "# fix_typo = list()\n",
    "# booknames = pd.read_csv(\"assets/book_names.csv\")\n",
    "\n",
    "# for i in range(len(df)):\n",
    "#     cleaned_report_str = str(df['Cleaned Report'][i:i+1].values[0])  # Convert DataFrame slice to string\n",
    "#     new_value = fix_bookname_typo(cleaned_report_str, booknames)\n",
    "#     # Remove double spaces\n",
    "#     output = ' '.join(new_value.split())\n",
    "#     fix_typo.append(output)\n",
    "\n",
    "# df['Fix Book Name'] = fix_typo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Fix Book Name'].to_csv('fix_bookname.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Just take the book name and chapter (bu saya sudah sampai kejadian 35, kejadian 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_list = pd.read_csv('assets/book_names.csv')\n",
    "book_list = book_list['Kitab'].tolist()\n",
    "\n",
    "# Construct a regex pattern to match any of the names in the list along with numbers and '-'\n",
    "pattern = r'(?:\\b(?:' + '|'.join(book_list) + r')\\b(?:\\s*\\d*-*\\d*\\s*)*)'\n",
    "\n",
    "processed_message = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    # Skip empty strings\n",
    "    text = df['Fix Book Name'][i:i+1].values[0]\n",
    "    if not text.strip():\n",
    "        continue\n",
    "    \n",
    "    # Find all matches for the pattern in the text\n",
    "    matches = re.findall(pattern, text)\n",
    "\n",
    "    if matches:\n",
    "        # Construct the final output by joining the matches\n",
    "        output = ' '.join(matches)\n",
    "        \n",
    "        # Remove double spaces\n",
    "        output = ' '.join(output.split())\n",
    "\n",
    "        # Remove space before or after hyphen\n",
    "        output = re.sub(r'\\s*-\\s*', r'-', output)\n",
    "    else:\n",
    "        output = text\n",
    "\n",
    "    processed_message.append(output)\n",
    "\n",
    "df['Processed'] = processed_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Processed'].to_csv('coba1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Change Abbreviation (Kel 2 - 3, Keluaran 2 - 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "singkatan_df = pd.read_csv('assets/singkatan.csv')\n",
    "kitab_df = pd.read_csv('assets/biblebooknames.csv')\n",
    "singkatan_dict = dict(zip(singkatan_df['Singkatan'], kitab_df['Kitab']))\n",
    "\n",
    "def ganti_singkatan(text):\n",
    "    if isinstance(text, str):  # Check if text is a string\n",
    "        words = text.split()\n",
    "        i = 0\n",
    "        while i < len(words):\n",
    "            word = words[i]\n",
    "            if '-' in word:\n",
    "                parts = word.split('-')\n",
    "                updated_parts = []\n",
    "                for part in parts:\n",
    "                    if part.lower() in singkatan_dict:  # Check lowercase for case-insensitivity\n",
    "                        updated_parts.append(singkatan_dict[part.lower()])\n",
    "                    else:\n",
    "                        updated_parts.append(part)\n",
    "                words[i] = '-'.join(updated_parts)\n",
    "            elif word.lower() in singkatan_dict:  # Check lowercase for case-insensitivity\n",
    "                words[i] = singkatan_dict[word.lower()]\n",
    "            elif i < len(words) - 1 and (word + ' ' + words[i+1]).lower() in singkatan_dict:  # Check for multi-word abbreviations\n",
    "                words[i] = singkatan_dict[(word + ' ' + words[i+1]).lower()]\n",
    "                del words[i+1]  # Remove the next word as it's part of the abbreviation\n",
    "            elif re.match(r'.*\\d', word):  # Check if the word contains a digit\n",
    "                break  # If a word with a digit is encountered, stop replacing\n",
    "            elif i > 0 and words[i-1].lower() in singkatan_dict:  # Check if the previous word is an abbreviation\n",
    "                words[i-1] = singkatan_dict[words[i-1].lower()]\n",
    "            i += 1\n",
    "        return ' '.join(words)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "processed_2 = list()\n",
    "\n",
    "for text in df['Processed']:\n",
    "    new_value = ganti_singkatan(text)\n",
    "    processed_2.append(new_value)\n",
    "\n",
    "df['Processed 2'] = processed_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Processed 2'].to_csv('coba2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_chapter_range(input_str):\n",
    "    # Split the input string into book and chapter range\n",
    "    parts = str(input_str).split()\n",
    "    if len(parts) > 2:\n",
    "        book = parts[0] + \" \" + parts[1]\n",
    "        chapters = parts[2].split('-')\n",
    "    elif len(parts) < 2:\n",
    "        return input_str\n",
    "    else:\n",
    "        book = parts[0]\n",
    "        chapters = parts[1].split('-')\n",
    "\n",
    "    # If there's only one chapter, return the original input\n",
    "    if len(chapters) == 1:\n",
    "        return input_str\n",
    "\n",
    "    # Otherwise, format the output\n",
    "    start_chapter = chapters[0]\n",
    "    end_chapter = chapters[1]\n",
    "    output = f\"{book} {start_chapter} - {book} {end_chapter}\"\n",
    "    return output\n",
    "\n",
    "\n",
    "processed3 = list()\n",
    "for i in range(len(df)):\n",
    "    new_value = expand_chapter_range(df['Processed 2'][i:i+1].values[0])\n",
    "    processed3.append(new_value)\n",
    "\n",
    "df['Processed 3'] = processed3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Processed 3'].to_csv('coba3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Message'].to_csv('cobacobaaja.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'rasul 2' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124massets/biblechapters.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     39\u001b[0m events \u001b[38;5;241m=\u001b[39m read_events(file_name)\n\u001b[0;32m---> 40\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mparse_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevents\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 19\u001b[0m, in \u001b[0;36mparse_messages\u001b[0;34m(df, events)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m match \u001b[38;5;129;01min\u001b[39;00m matches:\n\u001b[1;32m     18\u001b[0m     start_event, end_event \u001b[38;5;241m=\u001b[39m match\n\u001b[0;32m---> 19\u001b[0m     start_index \u001b[38;5;241m=\u001b[39m \u001b[43mevents\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     end_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(end_event) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_index, end_index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[0;31mValueError\u001b[0m: 'rasul 2' is not in list"
     ]
    }
   ],
   "source": [
    "# Fungsi untuk membaca jenis kejadian dan nomor kejadian dari file CSV\n",
    "def read_events(file_name):\n",
    "    events = []\n",
    "    with open(file_name, newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            events.append(row[0].lower())  # Tambahkan jenis kejadian dengan nomor kejadian\n",
    "    return events\n",
    "\n",
    "# Fungsi untuk mengubah format pesan\n",
    "def parse_messages(df, events):\n",
    "    parsed_messages = []\n",
    "\n",
    "    for line in df['Processed 3']:\n",
    "        matches = re.findall(r'(\\w+\\s\\d+)-(\\d+)', line)\n",
    "        parsed_line = ''\n",
    "        for match in matches:\n",
    "            start_event, end_event = match\n",
    "            start_index = events.index(start_event.lower())\n",
    "            end_index = int(end_event) - 1\n",
    "            for i in range(start_index, end_index + 1):\n",
    "                parsed_line += f\"{events[i].capitalize()}, \"\n",
    "        parsed_line = parsed_line[:-2]  # Menghapus koma dan spasi ekstra dari akhir\n",
    "        parsed_messages.append(parsed_line + \"\\n\")\n",
    "    \n",
    "    df['Parsed'] = parsed_messages\n",
    "\n",
    "    return df\n",
    "\n",
    "# Fungsi untuk menyimpan hasil parsing ke file\n",
    "def save_parsed_messages(parsed_messages, output_file):\n",
    "    with open(output_file, 'w') as file:\n",
    "        for parsed_line in parsed_messages:\n",
    "            file.write(parsed_line)\n",
    "\n",
    "# Contoh penggunaan\n",
    "file_name = 'assets/biblechapters.csv'\n",
    "\n",
    "events = read_events(file_name)\n",
    "df = parse_messages(df, events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
